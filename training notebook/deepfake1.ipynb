{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3134515,"sourceType":"datasetVersion","datasetId":1909705}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\n\n# Check for GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-17T13:33:00.319923Z","iopub.execute_input":"2024-10-17T13:33:00.320301Z","iopub.status.idle":"2024-10-17T13:33:00.383809Z","shell.execute_reply.started":"2024-10-17T13:33:00.320268Z","shell.execute_reply":"2024-10-17T13:33:00.382817Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define transformations for data augmentation and normalization\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T13:33:00.385385Z","iopub.execute_input":"2024-10-17T13:33:00.385721Z","iopub.status.idle":"2024-10-17T13:33:00.394187Z","shell.execute_reply.started":"2024-10-17T13:33:00.385687Z","shell.execute_reply":"2024-10-17T13:33:00.393308Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nimport random\nfrom pathlib import Path\n\n# Define the source directories\ntrain_dir = '/kaggle/input/deepfake-and-real-images/Dataset/Train'\nval_dir = '/kaggle/input/deepfake-and-real-images/Dataset/Validation'\ntest_dir = '/kaggle/input/deepfake-and-real-images/Dataset/Test'\n\n# Define temporary directories for downsampled data\ntemp_data_dir = '/kaggle/temp_downsampled_data'\ntemp_train_dir = os.path.join(temp_data_dir, 'Train')\ntemp_val_dir = os.path.join(temp_data_dir, 'Validation')\ntemp_test_dir = os.path.join(temp_data_dir, 'Test')\n\n# Function to copy a fraction of files from source to destination\ndef downsample_directory(source_dir, dest_dir, fraction=0.1):\n    Path(dest_dir).mkdir(parents=True, exist_ok=True)\n    for class_dir in os.listdir(source_dir):\n        full_class_dir = os.path.join(source_dir, class_dir)\n        dest_class_dir = os.path.join(dest_dir, class_dir)\n        Path(dest_class_dir).mkdir(parents=True, exist_ok=True)\n        \n        # Get all files in the current class directory and shuffle\n        files = os.listdir(full_class_dir)\n        random.shuffle(files)\n        \n        # Calculate the number of files to copy\n        num_files_to_copy = int(len(files) * fraction)\n        selected_files = files[:num_files_to_copy]\n        \n        # Copy the selected files to the new directory\n        for file in selected_files:\n            shutil.copy2(os.path.join(full_class_dir, file), os.path.join(dest_class_dir, file))\n\n# Downsample each dataset\ndownsample_directory(train_dir, temp_train_dir, fraction=0.25)\ndownsample_directory(val_dir, temp_val_dir, fraction=0.25)\ndownsample_directory(test_dir, temp_test_dir, fraction=0.25)\n\n# Load the downsampled dataset with ImageFolder\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Define transformations for data augmentation and normalization\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Load datasets with ImageFolder\ntrain_dataset = datasets.ImageFolder(root=temp_train_dir, transform=transform)\nval_dataset = datasets.ImageFolder(root=temp_val_dir, transform=transform)\ntest_dataset = datasets.ImageFolder(root=temp_test_dir, transform=transform)\n\n# Data loaders for the downsampled datasets\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# Verify downsampled subset sizes\nprint(f\"Downsampled Training dataset size: {len(train_dataset)}\")\nprint(f\"Downsampled Validation dataset size: {len(val_dataset)}\")\nprint(f\"Downsampled Test dataset size: {len(test_dataset)}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-17T13:33:00.395334Z","iopub.execute_input":"2024-10-17T13:33:00.395680Z","iopub.status.idle":"2024-10-17T13:38:25.044414Z","shell.execute_reply.started":"2024-10-17T13:33:00.395647Z","shell.execute_reply":"2024-10-17T13:38:25.043380Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downsampled Training dataset size: 35000\nDownsampled Validation dataset size: 9856\nDownsampled Test dataset size: 2726\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load a pre-trained model (Xception is not in torchvision; using ResNet here)\nmodel = models.resnet50(pretrained=True)\nmodel.fc = nn.Linear(model.fc.in_features, 1)  # Adjust for binary classification\n\n# Move the model to the GPU if available\nmodel = model.to(device)\nnum_gpus = torch.cuda.device_count()\nif num_gpus > 1:\n    model = nn.DataParallel(model)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T13:38:25.046434Z","iopub.execute_input":"2024-10-17T13:38:25.046749Z","iopub.status.idle":"2024-10-17T13:38:26.603428Z","shell.execute_reply.started":"2024-10-17T13:38:25.046716Z","shell.execute_reply":"2024-10-17T13:38:26.602633Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 153MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"# Define loss function and optimizer\ncriterion = nn.BCEWithLogitsLoss()  # Binary classification with logits\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T13:38:26.604531Z","iopub.execute_input":"2024-10-17T13:38:26.604857Z","iopub.status.idle":"2024-10-17T13:38:26.610544Z","shell.execute_reply.started":"2024-10-17T13:38:26.604823Z","shell.execute_reply":"2024-10-17T13:38:26.609539Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Assuming all previous setup (model, dataloaders, etc.) is already done\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport os\n# Define the directory to save checkpoints\ncheckpoint_dir = '/kaggle/working/'\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n# Training function with progress bar\ndef train_model(model, dataloader, criterion, optimizer, epoch):\n    model.train()\n    running_loss = 0.0\n    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\", unit=\"batch\")\n    for images, labels in progress_bar:\n        images, labels = images.to(device), labels.float().to(device).unsqueeze(1)\n        \n        # Zero the gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        progress_bar.set_postfix(train_loss=(running_loss / (progress_bar.n + 1)))\n        \n    return running_loss / len(dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T13:38:26.611747Z","iopub.execute_input":"2024-10-17T13:38:26.612024Z","iopub.status.idle":"2024-10-17T13:38:26.623216Z","shell.execute_reply.started":"2024-10-17T13:38:26.611993Z","shell.execute_reply":"2024-10-17T13:38:26.622403Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Validation function remains the same\ndef evaluate_model(model, dataloader, criterion):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in dataloader:\n            images, labels = images.to(device), labels.float().to(device).unsqueeze(1)\n            \n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            # Calculate accuracy\n            predicted = torch.round(torch.sigmoid(outputs))\n            correct += (predicted.cpu() == labels.cpu()).sum().item()\n            total += labels.size(0)\n            \n            running_loss += loss.item()\n    accuracy = 100 * correct / total\n    return running_loss / len(dataloader), accuracy","metadata":{"execution":{"iopub.status.busy":"2024-10-17T13:38:26.624264Z","iopub.execute_input":"2024-10-17T13:38:26.624631Z","iopub.status.idle":"2024-10-17T13:38:26.636179Z","shell.execute_reply.started":"2024-10-17T13:38:26.624593Z","shell.execute_reply":"2024-10-17T13:38:26.635390Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"num_epochs = 15\nbest_val_accuracy = 0.0\npatience = 3  # Number of epochs to wait for improvement\nepochs_no_improve = 0  # Counter for epochs without improvement\n\nfor epoch in range(num_epochs):\n    train_loss = train_model(model, train_loader, criterion, optimizer, epoch)\n    val_loss, val_accuracy = evaluate_model(model, val_loader, criterion)\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, \"\n          f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n    \n    # Check for improvement in validation accuracy\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        epochs_no_improve = 0  # Reset the counter\n        checkpoint_path = os.path.join(checkpoint_dir, f\"best_model_epoch_{epoch+1}.pth\")\n        torch.save(model.state_dict(), checkpoint_path)\n        print(f\"Checkpoint saved at {checkpoint_path}\")\n    else:\n        epochs_no_improve += 1  # Increment the counter\n        \n    # Check for early stopping\n    if epochs_no_improve >= patience:\n        print(f\"Early stopping triggered. No improvement for {patience} epochs.\")\n        break  # Exit the training loop\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T13:38:26.637194Z","iopub.execute_input":"2024-10-17T13:38:26.637493Z","iopub.status.idle":"2024-10-17T14:16:09.363157Z","shell.execute_reply.started":"2024-10-17T13:38:26.637462Z","shell.execute_reply":"2024-10-17T14:16:09.362084Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Epoch 1:   0%|          | 0/547 [00:00<?, ?batch/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\nEpoch 1: 100%|██████████| 547/547 [04:42<00:00,  1.94batch/s, train_loss=0.169]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/15], Train Loss: 0.1688, Validation Loss: 0.2488, Validation Accuracy: 90.64%\nCheckpoint saved at /kaggle/working/best_model_epoch_1.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 547/547 [04:41<00:00,  1.94batch/s, train_loss=0.0986]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/15], Train Loss: 0.0986, Validation Loss: 0.2041, Validation Accuracy: 90.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 547/547 [04:43<00:00,  1.93batch/s, train_loss=0.0781]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/15], Train Loss: 0.0781, Validation Loss: 0.3207, Validation Accuracy: 88.86%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 547/547 [04:47<00:00,  1.90batch/s, train_loss=0.0685]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/15], Train Loss: 0.0685, Validation Loss: 0.1302, Validation Accuracy: 95.03%\nCheckpoint saved at /kaggle/working/best_model_epoch_4.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 547/547 [04:41<00:00,  1.94batch/s, train_loss=0.0608]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/15], Train Loss: 0.0608, Validation Loss: 0.1446, Validation Accuracy: 94.25%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 547/547 [04:42<00:00,  1.94batch/s, train_loss=0.055] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/15], Train Loss: 0.0550, Validation Loss: 0.1283, Validation Accuracy: 94.93%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 547/547 [04:47<00:00,  1.90batch/s, train_loss=0.0502]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/15], Train Loss: 0.0502, Validation Loss: 0.1952, Validation Accuracy: 93.94%\nEarly stopping triggered. No improvement for 3 epochs.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test the model\ntest_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\nprint(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-10-17T14:16:09.364536Z","iopub.execute_input":"2024-10-17T14:16:09.364942Z","iopub.status.idle":"2024-10-17T14:16:19.956362Z","shell.execute_reply.started":"2024-10-17T14:16:09.364896Z","shell.execute_reply":"2024-10-17T14:16:19.955366Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Test Loss: 0.5442, Test Accuracy: 84.52%\n","output_type":"stream"}]},{"cell_type":"code","source":"from PIL import Image\nimport torch\nimport torch.nn.functional as F\nfrom torchvision import transforms\nimport torch.nn as nn\nfrom torchvision import models\n\n# Define the transformation pipeline (must match training preprocessing)\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Load the trained model and move it to the appropriate device\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice = torch.device(\"cpu\")\nprint(device)\ncheckpoint_path = '/kaggle/working/checkpoints/best_model_epoch_10.pth'  # Replace X with the appropriate epoch\nmodel = models.resnet50(pretrained=False)\nmodel.fc = nn.Linear(model.fc.in_features, 1)\nmodel.load_state_dict(torch.load(checkpoint_path, map_location=device))\nmodel = model.to(device)  # Move model to the appropriate device\nmodel.eval()\n\n# Define a function to classify an image as Fake or Real\ndef classify_image(image_path):\n    # Open the image file\n    img = Image.open(image_path).convert('RGB')\n    \n    # Apply the transformations\n    img_tensor = transform(img).unsqueeze(0)  # Add a batch dimension\n    \n    # Move the tensor to the appropriate device\n    img_tensor = img_tensor.to(device)\n    \n    # Perform inference\n    with torch.no_grad():\n        output = model(img_tensor)\n        prediction = torch.sigmoid(output).item()  # Get the probability\n    \n    # Classify based on the output\n    if prediction >= 0.5:\n        return \"Real\", prediction\n    else:\n        return \"Fake\", prediction\n\n# Example usage\nimage_path = '/kaggle/input/deepfake-and-real-images/Dataset/Test/Real/real_1002.jpg'\nlabel, confidence = classify_image(image_path)\nprint(f\"The image is classified as {label} with a confidence of {confidence:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T14:58:49.482130Z","iopub.execute_input":"2024-10-17T14:58:49.483020Z","iopub.status.idle":"2024-10-17T14:58:50.039807Z","shell.execute_reply.started":"2024-10-17T14:58:49.482978Z","shell.execute_reply":"2024-10-17T14:58:50.038506Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/1427928175.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mresnet50(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Move model to the appropriate device\u001b[39;00m\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/checkpoints/best_model_epoch_10.pth'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/checkpoints/best_model_epoch_10.pth'","output_type":"error"}]}]}